{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import shutil \n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin,ClassifierMixin\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import fbeta_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('data/housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(df['total_bedrooms'])\n",
    "df= df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_rooms']= df['total_rooms'] / df['households']\n",
    "df['num_bedroom']= df['total_bedrooms'] / df['households']\n",
    "df['pers_per_house']= df['population'] / df['households']\n",
    "\n",
    "df.drop(['total_bedrooms', 'total_rooms','population', 'households'], axis =1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['ocean_proximity'] == 'NEAR BAY', 'ocean_proximity'] = 1\n",
    "df.loc[df['ocean_proximity'] == 'NEAR OCEAN', 'ocean_proximity'] = 2\n",
    "df.loc[df['ocean_proximity'] == 'ISLAND', 'ocean_proximity'] = 3\n",
    "df.loc[df['ocean_proximity'] == '<1H OCEAN', 'ocean_proximity'] = 4\n",
    "df.loc[df['ocean_proximity'] == 'INLAND', 'ocean_proximity'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['housing_median_age','num_rooms','num_bedroom','median_income','ocean_proximity', 'pers_per_house']]\n",
    "y = df[\"median_house_value\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit & transform the train split and just transform the test split\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = y_scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_test_scaled = y_scaler.transform(np.array(y_test).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Rows, Columns:  (20433, 9)\n",
      "    longitude  latitude  housing_median_age  median_income  \\\n",
      "0     -122.23     37.88                41.0         8.3252   \n",
      "1     -122.22     37.86                21.0         8.3014   \n",
      "2     -122.24     37.85                52.0         7.2574   \n",
      "3     -122.25     37.85                52.0         5.6431   \n",
      "4     -122.25     37.85                52.0         3.8462   \n",
      "5     -122.25     37.85                52.0         4.0368   \n",
      "6     -122.25     37.84                52.0         3.6591   \n",
      "7     -122.25     37.84                52.0         3.1200   \n",
      "8     -122.26     37.84                42.0         2.0804   \n",
      "9     -122.25     37.84                52.0         3.6912   \n",
      "10    -122.26     37.85                52.0         3.2031   \n",
      "11    -122.26     37.85                52.0         3.2705   \n",
      "12    -122.26     37.85                52.0         3.0750   \n",
      "13    -122.26     37.84                52.0         2.6736   \n",
      "14    -122.26     37.85                52.0         1.9167   \n",
      "\n",
      "    median_house_value ocean_proximity  num_rooms  num_bedroom  pers_per_house  \n",
      "0             452600.0               1   6.984127     1.023810        2.555556  \n",
      "1             358500.0               1   6.238137     0.971880        2.109842  \n",
      "2             352100.0               1   8.288136     1.073446        2.802260  \n",
      "3             341300.0               1   5.817352     1.073059        2.547945  \n",
      "4             342200.0               1   6.281853     1.081081        2.181467  \n",
      "5             269700.0               1   4.761658     1.103627        2.139896  \n",
      "6             299200.0               1   4.931907     0.951362        2.128405  \n",
      "7             241400.0               1   4.797527     1.061824        1.788253  \n",
      "8             226700.0               1   4.294118     1.117647        2.026891  \n",
      "9             261100.0               1   4.970588     0.990196        2.172269  \n",
      "10            281500.0               1   5.477612     1.079602        2.263682  \n",
      "11            241800.0               1   4.772480     1.024523        2.049046  \n",
      "12            213500.0               1   5.322650     1.012821        2.346154  \n",
      "13            191300.0               1   4.000000     1.097701        1.982759  \n",
      "14            159200.0               1   4.262903     1.009677        1.954839  \n"
     ]
    }
   ],
   "source": [
    "featcols = {\n",
    "  colname : tf.feature_column.numeric_column(colname) \\\n",
    "    for colname in 'housing_median_age,median_income,num_rooms,num_bedroom,pers_per_house,ocean_proximity'.split(',')\n",
    "}\n",
    "# Bucketize lat, lon so it's not so high-res; California is mostly N-S, so more lats than lons\n",
    "featcols['longitude'] = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'),\n",
    "                                                   np.linspace(-124.3, -114.3, 5).tolist())\n",
    "featcols['latitude'] = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'),\n",
    "                                                  np.linspace(32.5, 42, 10).tolist())\n",
    "\n",
    "# Split into train and eval\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "traindf = df[msk]\n",
    "evaldf = df[~msk]\n",
    "\n",
    "SCALE = 100000\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x = traindf[list(featcols.keys())],\n",
    "                                                               y =(traindf[\"median_house_value\"]/100),\n",
    "                                                               batch_size= BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "eval_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x = evaldf[list(featcols.keys())],\n",
    "                                                    y = evaldf[\"median_house_value\"] / SCALE,  # note the scaling\n",
    "                                                    num_epochs = 10, \n",
    "                                                    batch_size = len(evaldf), \n",
    "                                                    shuffle=False)\n",
    "print('# of Rows, Columns: ',df.shape)\n",
    "print(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 100000\n",
    "def print_rmse(model, name, input_fn):\n",
    "    metrics=model.evaluate(input_fn=input_fn, septs=1)\n",
    "    print(\"RMES sur {}dataset={} USD\".format(name, np.sqrt(metrics['average_loss'])*SCALE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "with open(\"data/X_train_scaled.pickle\", 'rb') as xtrain, open(\"data/y_train_scaled.pickle\", 'rb') as ytrain:\n",
    "    X_train, y_train = pickle.load(xtrain), pickle.load(ytrain)\n",
    "\n",
    "# Test\n",
    "with open(\"data/X_test_scaled.pickle\", 'rb') as xtest, open(\"data/y_test_scaled.pickle\", 'rb') as ytest:\n",
    "    X_test, y_test = pickle.load(xtest), pickle.load(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BR(BaseEstimator,RegressorMixin):\n",
    "\n",
    "    # instantiation values\n",
    "    def __init__(self,lamd=1.0e-5,alph=1e-5,maxiter=2000,rtol=1.0e-5,verbose=True):    \n",
    "        self.maxiter = maxiter # class contains only tunable hyperparameters (max convergence iteration)\n",
    "        self.rtol = rtol       # convergence tolerance for hyperparameters\n",
    "        self.lamd = lamd   # hyperparameter \n",
    "        self.alph = alph     # hyperparameter\n",
    "        self.verbose = verbose # can be activated to check coverged hyperparameters\n",
    "    \n",
    "    # compute mean cofficients/covariance matrix of posterior mean\n",
    "    @staticmethod\n",
    "    def posterior(X,y,lamd,alph):\n",
    "        ndim = X.shape[1]\n",
    "        S_N_inv = lamd * np.eye(ndim) + alph * X.T.dot(X) \n",
    "        S_N = inv(S_N_inv)                                      \n",
    "        m_N = alph * S_N.dot(X.T).dot(y)                \n",
    "        return m_N, S_N\n",
    "    \n",
    "    ''' train a bayesian ridge regression model + nearest classification '''\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "\n",
    "        ''' A. Check Input Data Copatibility '''\n",
    "        if(type(X) is np.ndarray):\n",
    "            self.X = X;self.y = y\n",
    "        else:\n",
    "            self.X = X.values; self.y = y.values\n",
    "        ntot,ndim = self.X.shape\n",
    "\n",
    "        # set initial value for hyperparameters\n",
    "        eig0 = np.linalg.eigvalsh(self.X.T.dot(self.X))  # diagonal component (ndim,)\n",
    "\n",
    "        # tune hyperparameters via convergence tolerance.\n",
    "        for niter in range(self.maxiter):\n",
    "\n",
    "            alph1 = self.alph\n",
    "            lamd1 = self.lamd\n",
    "            eig = eig0*self.alph\n",
    "\n",
    "            # make prediction on training data\n",
    "            self.m_N, self.S_N = self.posterior(self.X,self.y,self.lamd,self.alph)\n",
    "\n",
    "            gamma = np.sum(eig/(eig+self.lamd))\n",
    "            self.lamd = gamma / np.sum(self.m_N ** 2)\n",
    "            Ibeta = 1.0 / (ntot-gamma) * np.sum((self.y - self.X.dot(self.m_N)) ** 2)\n",
    "            self.beta = 1.0/Ibeta\n",
    "\n",
    "            # define exit condition\n",
    "            if np.isclose(lamd1,self.lamd,self.rtol) and np.isclose(alph1,self.alph,self.rtol):\n",
    "                if(self.verbose is True):\n",
    "                    print(f'{self.rtol} achieved at {niter+1} iterations.')\n",
    "                    print(f'Converged Hyperparameters: {self.lamd,self.alph}')\n",
    "                return self\n",
    "\n",
    "        return self\n",
    "\n",
    "    ''' make new predictions; mean + variance of posterior predictive distribution '''\n",
    "    \n",
    "    def predict(self,X):\n",
    "        if(type(X) is np.ndarray):\n",
    "            self.X = X\n",
    "        else:\n",
    "            self.X = X.values\n",
    "        self.mu_s = X.dot(self.m_N)\n",
    "        self.cov_s = 1.0 / self.alph + np.sum(X.dot(self.S_N) * X, axis=1)\n",
    "        return self.mu_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation w/ Cross Validation\n",
    "def modelEval(ldf,feature='median_house_value',model_id = 'dummy'):\n",
    "    \n",
    "    # Input: Feature & Target DataFrame\n",
    "\n",
    "    # Split feature/target variable\n",
    "    y = df[feature].copy()\n",
    "    X = df.copy()\n",
    "    del X[feature]\n",
    "   # remove target variable\n",
    "    \n",
    "    # Pick Model \n",
    "   #if(model_id is 'dummy'):  \n",
    "    model = DummyRegressor()\n",
    "   #if(model_id is 'br'):   \n",
    "    #model = BR(verbose=False)  \n",
    "    #model = RandomForestRegressor(n_estimators=10,random_state=10)\n",
    "    \n",
    "    ''' Parameter Based Cross Validation (No Pipeline)'''\n",
    "    #gscv = GridSearchCV(model,param_grid,cv=5)\n",
    "    #gscv.fit(X,y)\n",
    "    #results = pd.df(gscv.cv_results_)\n",
    "    #scores = np.array(results.mean_test_score).reshape(7,7)\n",
    "    \n",
    "#     # plot the cross validation mean scores\n",
    "    #heatmap1(scores,xlabel='lamda',xticklabels=param_grid['lamd'],\n",
    "          #   ylabel='alpha',yticklabels=param_grid['alph'])\n",
    "    \n",
    "    ''' Standard Cross Validation '''\n",
    "    cv_score = np.sqrt(-cross_val_score(model,X,y,cv=5,scoring='neg_mean_squared_error'))\n",
    "    print(\"Scores:\",cv_score);\n",
    "    print(\"Mean:\", cv_score.mean());\n",
    "    print(\"std:\", cv_score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [114361.51883352 109621.68676131 125662.15311405 112298.79716652\n",
      " 122776.84571602]\n",
      "Mean: 116944.20031828254\n",
      "std: 6195.024974465456\n"
     ]
    }
   ],
   "source": [
    "trdata= ['X_train, X_test, y_train, y_test']\n",
    "modelEval(trdata, model_id='dummy')\n",
    "#print('Score: '+ str(r2_score(y,X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['median_house_value'], axis=1)\n",
    "y = df['median_house_value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.33,\n",
    "                                                   random_state=42)\n",
    "#  splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression() #  importing LinearRegression\n",
    "\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:205677.60803796528\n",
      "prediction: 206280.9631702541\n"
     ]
    }
   ],
   "source": [
    "print(f'actual:{y_test.mean()}')\n",
    "print(f'prediction: {predictions.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 52360.19190552398\n",
      "MSE: 5174333830.004562\n",
      "Score: 0.6097593196371391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print('MAE: ' + str(mean_absolute_error(y_test, predictions)))\n",
    "print('MSE: ' + str(mean_squared_error(y_test, predictions)))\n",
    "print('Score: '+ str(r2_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.06678104, 0.03797865, 0.03697824, 0.03498077, 0.03298163,\n",
       "        0.02998328, 0.02298689, 0.02498651]),\n",
       " 'score_time': array([0.01599097, 0.00999427, 0.00999355, 0.00999331, 0.00899482,\n",
       "        0.00699544, 0.00799489, 0.0059967 ]),\n",
       " 'test_r2': array([0.38451201, 0.52563889, 0.52378341, 0.56264329, 0.61075479,\n",
       "        0.4400234 , 0.45522649, 0.72493079]),\n",
       " 'test_e2': array([5.84429972e+09, 6.25733707e+09, 5.95505789e+09, 6.44680767e+09,\n",
       "        4.58250875e+09, 4.47184498e+09, 7.73444381e+09, 3.96144229e+09])}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(reg, X, y, cv=8, scoring=dict(r2=make_scorer(r2_score), e2=make_scorer(mean_squared_error)),\n",
    "              return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "Reg_model= RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual mean:205677.60803796528\n",
      "Prediction mean: 206253.72087349842\n"
     ]
    }
   ],
   "source": [
    "actual = y_test\n",
    "predictions= Reg_model.predict(X_test)\n",
    "print(f'Actual mean:{np.mean(actual)}')\n",
    "print(f'Prediction mean: {np.mean(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 32338.340965445648\n",
      "MSE: 2496663663.0347157\n",
      "Score: 0.811705282552471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print('MAE: ' + str(mean_absolute_error(y_test, predictions)))\n",
    "print('MSE: ' + str(mean_squared_error(y_test, predictions)))\n",
    "print('Score: '+ str(r2_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([12.83272982, 12.03180861, 12.00187969, 10.8289535 , 11.16567826]),\n",
       " 'score_time': array([0.07813883, 0.08726931, 0.09874892, 0.07812381, 0.09374785]),\n",
       " 'test_r2': array([0.4398467 , 0.64595424, 0.73888425, 0.4451187 , 0.72533808]),\n",
       " 'test_e2': array([6.02211259e+09, 4.15206530e+09, 3.76569002e+09, 6.51764046e+09,\n",
       "        3.98682880e+09])}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(Reg_model, X, y, cv=5, scoring=dict(r2=make_scorer(r2_score), e2=make_scorer(mean_squared_error)),\n",
    "              return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.811705282552471\n"
     ]
    }
   ],
   "source": [
    "print('Score: '+ str(r2_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from keras.layers.core import Dense\n",
    "\n",
    "# # define the model\n",
    "# def larger_model():\n",
    "#     # create model\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(10, input_dim=10, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(5, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "#     # Compile model\n",
    "#     model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#     return model\n",
    "\n",
    "def build_and_compile_model(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_model(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             0\n",
       "latitude              0\n",
       "housing_median_age    0\n",
       "median_income         0\n",
       "ocean_proximity       0\n",
       "num_rooms             0\n",
       "num_bedroom           0\n",
       "pers_per_house        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# train_features = X_train\n",
    "# test_features = X_test\n",
    "\n",
    "# train_labels=y_train\n",
    "# test_labels=y_test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit & transform the train split and just transform the test split\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = y_scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_test_scaled = y_scaler.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "house_normalizer = X_scaler, y_scaler\n",
    "house_normalizer.adapt(house)\n",
    "# layer= layers.Normalization()\n",
    "# layer.adapt(house)\n",
    "# normalized_data = layer(house)\n",
    "\n",
    "house_model = tf.keras.Sequential([\n",
    "    house_normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "house_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dnn_model = build_and_compile_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dnn_house_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!conda install LSTM \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, activation='relu',\n",
    "               input_shape=(1000, 1), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "history = dnn_house_model.fit(\n",
    "    train_features['housing_median_age','median_income','num_rooms','num_bedroom','pers_per_house','ocean_proximity'],\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(0.0, 250, 251)\n",
    "y = dnn_house_model.predict(x)\n",
    "\n",
    "plot_house(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['dnn_house_model'] = dnn_house_model.evaluate(\n",
    "    test_features['median_house_value'], test_labels,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = build_and_compile_model(normalizer)\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = dnn_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = dnn_model.predict(test_features).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Prediction Error [MPG]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model.save('dnn_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=10, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=8)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "#print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}